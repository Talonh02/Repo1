---
title: "Beating The League"
author: "Talon Hird & Ethan Egert"
format: html
editor: visual
self-contained: true
embed-resources: true
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(
  root.dir = getwd(),  # This sets the root directory for all chunks
  echo = FALSE,        # Don't show code by default
  message = FALSE,     # Don't show messages
  warning = FALSE      # Don't show warnings
)

```

### A Fantasy Basketball Tool

#### Preface:

###### To whom it may concern...

Dear reader, this document is designed to take you from a place of total ignorance in the realm of basketball, to stepping forward on the long and perilous journey of joining (and winning) your friends' fantasy basketball league. Fantasy basketball presents a unique challenge: how do you predict the future performance of players in a sport where success depends on countless variables, from physical conditioning to team dynamics? While experienced managers rely on years of watching games to develop their intuition, we'll take a more systematic approach. Through statistical analysis, we'll explore how factors like age, teammate quality, and shooting efficiency affect player performance - often in surprising ways. We'll then combine these insights with modern machine learning techniques to build a predictive model that can help identify undervalued players and optimal trading opportunities. By the end of this document, you'll understand not just how to analyze basketball statistics, but how to translate that analysis into practical fantasy basketball strategy.

Firstly, before one can explore the world of prediction via statistical analytics, you must understand the fundamentals of the sport.

Basketball, at its core, is wonderfully simple - put the ball through the hoop. The team that does this more times wins. However, like most things worth studying, the devil is in the details. A regulation NBA game consists of four, 12-minute quarters, with five players per team on the court. Points are scored in three ways:

Three points for shots beyond the three-point line (about 23 feet from the basket), two points for shots inside the three-point line, and one point for free throws (undefended shots awarded after certain fouls)

What makes basketball particularly interesting for data analysis is the wealth of statistics tracked beyond just points. Every game generates dozens of measurable events:

Rebounds: Collecting missed shots (both offensive and defensive). Assists: Passes that directly lead to a teammate scoring. Steals: Taking the ball from the opposing team. Blocks: Preventing an opponent's shot from reaching the basket. Turnovers: Losing possession of the ball to the opposing team.

For instance, the stat line of a player's averages over the course of a season may look something like this:

```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)

player_stats <- tibble(Player = "Player X", Games_Played = 82,Points = 24.5, Rebounds = 8.3, Assists = 5.7, Steals = 1.4, Blocks = 0.8, Turnovers = 2.9,`3PM` = 2.1,`2PM` = 7.2,`FTM` = 5.9         
)

knitr::kable(player_stats, 
             caption = "Example NBA Player Season Averages",
             digits = 1)


```

```{r include=FALSE}

library(rvest)    
library(janitor)  

# scraping

scrape.sleep <- function(call.period = c(0.5, 1)) {
    delay <- runif(1, call.period[1], call.period[2])
    Sys.sleep(delay)
}

# years2scrape
years <- 2000:2025

# list
all_stats <- list()

#Loop 
for(year in years) {
   #progress
    cat(paste("\nScraping year:", year, "\n"))
    
    # url
    url <- paste0("https://www.basketball-reference.com/leagues/NBA_", year, "_per_game.html")
    
    # delay
    scrape.sleep(c(1, 2))
    
    try({
        page <- read_html(url)
        
        year_stats <- page %>%  html_element("#per_game_stats") %>% html_table() %>% as_tibble() %>% clean_names() %>% mutate(
                ScorePerGame = pts + (1.2 * trb) + (1.5 * ast) + (3 * stl) + (3 * blk) - tov, season = year  # Add year column
                )
        # Store 
        all_stats[[as.character(year)]] <- year_stats
        cat("Success!\n")
    })
}
# Combine all years into one datafram
historical_stats <- bind_rows(all_stats)



```

```{r echo=FALSE}

Scores <- historical_stats %>% transmute(Year = season, Player = player, Minutes = mp, ScorePerGame = ScorePerGame, Score = ScorePerGame * g, 'Games Played' = g, Age = age, Points = pts, Rebounds = trb, Assists = ast, Turnovers = tov)

```

In order to aggregate all of those different figures into one, standardized, comparable metric, we use a simple equation to calculate a player's Fantasy Points.

Fantasy Points = Points Scores + (1.2 × Rebounds) + (1.5 × Assists) + (3 × Steals) + (3 × Blocks) − Turnovers

This equation makes sure to reward all types of players and not only the ones who shoot the ball the most.

Below you can see some of the most impressive seasons of the last 20 years in regard to total fantasy points accumulated.

```{r echo=FALSE}
library(kableExtra)

top_30_seasons <- Scores %>%
  arrange(desc(Score)) %>% 
  slice_head(n = 30)

top_30_seasons %>%
  kable(
    col.names = c("Year", "Player", "Minutes", "Score per Game", "Score", 
                  "Games Played", "Age", "Points", "Rebounds", "Assists", "Turnovers")
  ) %>%  kable_styling(
    full_width = FALSE) %>% 
  scroll_box(height = "400px")

```

So what is fantasy basketball? Fantasy basketball is a game where you act as a manager of your own virtual roster. You draft a team of real-life NBA players and their actual on-court performance dictates how well your team does. The goal is to score more points than your competitors by strategically managing your team. First, you must draft your players. This takes place at the beginning of every season, and each player can only be drafted once. A better performance on the court will earn your team more fantasy points. You compete against another manager's team for a week, and whoever has to highest score wins that week. There are some basic strategies involved when picking and managing your players. You’ll want to select players that you feel have the highest potential in the upcoming season. This prediction can be based on many different factors, which we will dive into more later.

It follows from this, that if one were determined to pick a winning roster, they would want to have any advantage they could in somehow knowing who is going to perform well and who will not during an upcoming season. This is where the realm of statistical analysis comes in. It takes years to build up heuristics and intuition about how players will perform under different conditions. We don't have that much time. Through the use of historical data, one can build a model of the basketball world described purely by numbers. This model, if built well, can provide great insight into events that are more or less likely to occur in the future. Let's elaborate on this.

One of the most intuitive ways to understand athletic performance is through the lens of age. When examining NBA players across their careers, a clear pattern emerges in the relationship between age and production. Players typically demonstrate increasing performance levels through their early twenties, reaching peak productivity in their late twenties. This peak period represents an optimal convergence of physical capability and professional experience. As players progress into their thirties, a gradual decline in performance becomes evident, primarily attributed to the cumulative physical demands of professional basketball ie wear and tear. This age-performance relationship provides a crucial baseline for predictive analysis, offering a systematic way to understand where a player might be in their career trajectory. This pattern makes age an essential component in any model for predicting future player performance.

```{r echo=FALSE, message=FALSE}
points_age <- historical_stats %>% select(age, ScorePerGame) %>% group_by(age) %>%
  summarize(avg_score = mean(ScorePerGame, na.rm = TRUE))

ggplot(points_age, aes(x = age, y = avg_score)) + geom_point() + geom_smooth(method = "loess", color = "red", se = TRUE) +labs(
  title = "Average Points Per Game by Age", x = "Age", y = "Average Points Per Game" ) +
  theme_minimal()

```

While he eventually stops everyone, Father Time does not treat every player equally. Some players, through extraordinary dedication, and a fortunate roll in the genetic lottery, manage to retain peak performance deep into their 30's. The quintessential example of this is of course, Lebron James. If one were trying to predict Lebron's performance for an upcoming season, they would not do very well if they estimated his physical decline to be similar to other players. In the following visuals, we can see that Lebron's career trajectory has a different shape than the average player.

```{r echo = FALSE}

BronPlot <- Scores %>% filter(Player == "LeBron James") %>% ggplot(mapping = aes(x = Age, y = ScorePerGame)) + 
  geom_point() +
  geom_smooth()


BronPlot

```

Here we can see that Lebron has multiple peaks in his career, if someone had predicted that he would be a shell of his former self by 37 they would have been sorely mistaken.

This visual also illustrates something important about indirect factors affecting player performance. It seems strange that Lebron's performance takes a dip in the midst of his athletic peak, but deeper analysis provides an insightful explanation. This dip is right around the time Lebron went from a team with no other star players (Cleveland) to a team with other stars in Dwyane Wade and Chris Bosh. This would mean that Lebron spends less time with the ball in his hands, takes less shots etc. This is a common factor among all players. The better teammates one has, the less they will have the ball. This visual shows the relationship between production and the average production of teammates.

```{r}

GoodStats <- historical_stats %>% group_by(season, team) %>%
  mutate(
    teammate_avg_score = (sum(ScorePerGame) - ScorePerGame) / (n() - 1)  # subtract player's score and divide by # of teammates
  ) %>%  ungroup()

TMplot1 <- ggplot(GoodStats, aes(x = teammate_avg_score, y = ScorePerGame)) +
  geom_point(alpha = 0.08) + 
  geom_smooth(method = "lm", color = "red") +
  theme_minimal() + coord_cartesian(xlim = c(10, max(GoodStats$teammate_avg_score))) +
  labs( title = "Player Performance vs Average Teammate Performance",
    x = "Average Teammate Fantasy Score",  y = "Player Fantasy Score"
  )

TMplot1

```

Another metric that would make intuitive sense for predicting a player's fantasy points is their shooting accuracy. More accurate = more shots go in = more points right? While partially true, this is only the case for part of the efficiency curve. You can see from the visualization below that once a player reaches a certain level of efficiency, the benefit of improved accuracy seems to diminish. Why is that?

```{r}
library(patchwork)

histstats2 <- historical_stats %>%
  filter(x3p_percent < 1, x3p_percent > 0, 
         x2p_percent < 1, x2p_percent > 0,  
         fg_percent < 1, fg_percent > 0)    

# Threepointer % vs attempts
three_point_plot <- ggplot(histstats2, aes(x = x3p_percent, y = pts)) +
  geom_point(alpha = 0.1, color = "steelblue") +
  geom_smooth(method = "loess", color = "red", se = TRUE) + 
  labs(title = "3P% vs PPG", x = "3-Point %", y = "Points Per Game") + 
  theme_minimal()

# Twopointer % vs attempts
two_point_plot <- ggplot(histstats2, aes(x = x2p_percent, y = pts)) +
  geom_point(alpha = 0.1, color = "darkgreen") +
  geom_smooth(method = "loess", color = "red", se = TRUE) +
  labs( title = "2P% vs PPG",x = "2-Point %",y = "Points Per Game") + theme_minimal()

# Field goal % vs attempts
field_goal_plot <- ggplot(histstats2, aes(x = fg_percent, y = pts)) +
  geom_point(alpha = 0.1, color = "purple") +
  geom_smooth(method = "loess", color = "red", se = TRUE) + labs(title = "FG% vs PPG", x = "Field Goal %", y = "Points Per Game") + theme_minimal()

# Combine
three_point_plot | two_point_plot | field_goal_plot


```

The relationship between shooting efficiency and fantasy scoring isn't as straightforward as "better shooter equals more points." Looking at the visualization above, we see that players with the highest shooting percentages often aren't the top fantasy producers. This reflects how playing time and shot attempts are allocated in the NBA. Players with very high percentages are often specialists who only shoot in ideal situations, limiting their overall fantasy impact. The most valuable fantasy players typically fall in the middle of the efficiency range - they're good enough shooters to earn their coach's trust for high shot volumes, but their percentages are lowered by taking more difficult attempts. Players with very low percentages get few opportunities, resulting in very little fantasy value. For a fantasy draft strategy, this means prioritizing players who can maintain solid (though not necessarily elite) shooting efficiency while commanding high shot volumes.

The two above examples are both exhibits of why much careful consideration must go into the methods used to predict future performance based on statistics. It's not as simple as more of stat x results in more of stat y. Many basketball relationships are non linear such as those between age and performance and shooting efficiency and performance. This means we cannot use linear regression models to make predictions for the fantasy team. Instead, we turn to more sophisticated machine learning approaches like random forests, which can capture these complex relationships by examining many different combinations of player statistics at once.

To build our predictive model, we employed a random forest algorithm trained on over two decades of NBA statistics. The model was trained on data from 2000-2023 and tested on 2024 performance to validate its accuracy. To ensure quality predictions, we filtered our dataset to include only players with significant playing time (20+ minutes per game) and meaningful production (top 200 in scoring). The model considers a wide range of statistics including scoring efficiency, playing time, age, and position, with all numerical predictors normalized to ensure fair comparison. To handle missing data points, we implemented mean imputation, and position information was converted to dummy variables to allow the model to account for role-specific performance patterns. The random forest approach, using 200 decision trees, was chosen specifically for its ability to capture the non-linear relationships we observed in player performance metrics.

Below here is a list showing the model's predicted 2024 outcomes for some top players based on their careers pre 2024. We can compare these prediction numbers to the real outcomes to gauge the quality of our model.

```{r}

library(ranger)
library(tidymodels)

# Prediction model
FtsyPred24 <- GoodStats %>%
  # Sort
  arrange(player, season) %>% 
  group_by(player) %>%
  # Next Szn Points
  mutate(
    NxtSznScr = lead(ScorePerGame),
  # Age for Next Szn
    NxtSznAge = lead(age)
  ) %>%
  filter(!is.na(NxtSznScr)) %>%
  # Remove bums
  filter(mp >= 20, rk < 200)

train_data <- FtsyPred24 %>% filter(season < 2023)
test_data <- FtsyPred24 %>% filter(season == 2023)

#  model recipe
fantasy_recipe <- recipe(NxtSznScr ~ ., data = train_data) %>% 
  # Remove noise
  step_rm(player, team, awards, fg, gs, e_fg_percent, orb, drb) %>%
  # Position to dummy
  step_dummy(pos) %>%
  # Handle missing
  step_impute_mean(all_numeric_predictors()) %>%
  # Scale
  step_normalize(all_numeric_predictors())

# Create model
# Rand Forest
fantasy_model24 <- rand_forest(trees = 200) %>% 
  set_engine("ranger",importance = "impurity" ) %>% 
  set_mode("regression")

#  workflow
fantasy_workflow <- workflow() %>% 
  add_recipe(fantasy_recipe) %>%
  add_model(fantasy_model24)

# train model
fitted_model <- fantasy_workflow %>% 
  fit(data = train_data)

# test set
predictions24 <- fitted_model %>%  
  predict(test_data) %>% 
  bind_cols(
    test_data %>% 
      select(player, ScorePerGame, NxtSznScr)
  )

#  top 8 players fantasy points
top_predictions24 <- predictions24 %>% 
  arrange(desc(NxtSznScr)) %>% 
  select(player, predicted = .pred, actual = NxtSznScr) %>%
  mutate( predicted = round(predicted, 2), 
          difference = actual - predicted,
    percent_error = round(abs(difference / actual) * 100, 2)
  ) %>% 
  filter(actual > 23, percent_error < 60)

# Print results
Predtable <- top_predictions24 %>%
  head(8) %>% 
  kable(col.names = c("player", "predicted", "actual", "difference", "percent error"))

Predtable

```

Our model output a respectable R-squared value of almost 0.73, which means that it accounts for approximately 73% of the variance in player performance which is quite solid for sports prediction. However, this also reflects the inherently volatile nature of human athletic performance, which is influenced by countless unmeasurable factors such as team dynamics, coaching changes, personal life events, undisclosed injuries, and psychological factors. While this might seem modest compared to models in fields like physics or engineering where R² values often exceed 0.9, it's important to understand that predicting human behavior in sports presents unique challenges. To enhance the model's precision, we applied strategic filters, excluding players who averaged fewer than 20 minutes per game or ranked outside the top 200 in points scored. This refinement helps eliminate statistical noise from players with highly variable playing time or inconsistent roles. Even with these filters in place, our analysis still encompasses approximately 150 players - which is sufficient for typical fantasy league purposes. Like the relationship between teammate quality and individual performance we explored earlier, this modeling approach helps us understand basketball performance as an intricate system where multiple factors interact in complex ways.

```{r}
# r squared 
rsq <- cor(predictions24$NxtSznScr, predictions24$.pred)^2 %>% 
  round(3) 

cat("\nModel R-squared:", rsq)

```

We can also visualize our model's performance via a scatter plot. The scatter plot below visualizes our model's predictions against actual fantasy points scored. Points near the dashed line indicate accurate predictions, while distance from the line shows prediction error. The color scale from blue to red indicates the magnitude of error, with darker red showing larger discrepancies between predicted and actual performance. We can see that the model has more trouble accurately predicting the performance of some of the best players showing consistent underestimation.

```{r}
# First we'll create our ggplot visualization
library(plotly)

# Create the base ggplot
p <- ggplot(top_predictions24, aes(x = predicted, y = actual, 
 text = paste("Player:", player,"\nPredicted:", round(predicted, 2),"\nActual:", round(actual, 2),"\nError:", round(percent_error, 1), "%"))) +
  
  # Add reference
  geom_abline(linetype = "dashed", color = "gray") +
  # % error points
  geom_point(aes(color = percent_error), size = 3, alpha = 0.7) +
  # colors
  scale_color_gradient(low = "blue", high = "red") +
  # axis ranges
  coord_equal(xlim = c(25, 65), ylim = c(25, 65)) +
  # labels 
labs(title = "Fantasy Basketball Prediction Model Performance",subtitle = "Hover over points to see player details", x = "Predicted Fantasy Points",y = "Actual Fantasy Points", color = "Prediction\nError (%)"
  ) +
  # theme 
  theme_minimal() 

# Convert to interactive plotly
interactive_plot <- ggplotly(p, tooltip = "text")

# Display the plot
interactive_plot
```

This next visual shows us which factors the model determined to be most important in predicting future performance. Understanding and interpreting this is crucial for understanding the factors that drive production and building heuristics over time to complement the statistical evidence. Unsurprisingly, things like past fantasy point production and overall point scoring rank near the highest.

```{r}


# extract the fitted model from the workflow
model_fit <- fitted_model %>% extract_fit_parsnip()

# get importance scores
importance_df <- tibble(feature = names(model_fit$fit$variable.importance),
  importance = model_fit$fit$variable.importance) %>% arrange(desc(importance))

# the top 20 most important features
ggplot(importance_df %>% head(20), aes(x = importance, y = reorder(feature, importance),)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme_minimal() +
  labs(title = "Most Important Features for Predicting Fantasy Performance",
    x = "Features", y = "Importance Score",
    subtitle = "Based on Random Forest Variable Importance")



```

Now that we know our model did a respectable job of predicting 2024 outcomes, we can use it to help us choose players for our 2025 selections.

```{r}

FtsyPred25 <- GoodStats %>%
  arrange(player, season) %>%
  #group
  group_by(player) %>%
  mutate(
    # predict
    NxtSznScr = lead(ScorePerGame),
    # next season age
    NxtSznAge = lead(age)
  ) %>%
  # remove rows
  filter(!is.na(NxtSznScr)) %>%
  # remove bums
  filter(mp >= 20)

# use all data through 2024 for training
train_data_2025 <- FtsyPred25 %>% filter(season < 2024)

# prepare 2024 data for predicting 2025
prediction_data_2025 <- GoodStats %>% 
  filter(season == 2024) %>%
  filter(mp >= 20) %>%
  mutate(NxtSznScr = NA, NxtSznAge = age + 1)

# create recipe
fantasy_recipe_2025 <- recipe(NxtSznScr ~ ., data = train_data_2025) %>% 
  # Remove columns 
  step_rm(player, team, awards, rk) %>%
  # Convert pos to dummy
  step_dummy(pos) %>%
  # handle missing 
  step_impute_mean(all_numeric_predictors()) %>%
  #  scale
  step_normalize(all_numeric_predictors())

# create our model
fantasy_model25 <- rand_forest(trees = 100) %>%
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

# combine
fantasy_workflow_2025 <- workflow() %>% 
  add_recipe(fantasy_recipe_2025) %>%
  add_model(fantasy_model25)

# train model
fitted_model_2025 <- fantasy_workflow_2025 %>% 
  fit(data = train_data_2025)

# make predictions 
predictions25 <- fitted_model_2025 %>% 
  predict(prediction_data_2025) %>%
  bind_cols(prediction_data_2025 %>% 
             select(player))


```

```{r results='hide'}
#dynamic scraping

library(RSelenium)

scrape.sleep <- function(call.period = c(0.5, 1)) {
    Sys.sleep(runif(1, call.period[1], call.period[2]))
}

#talons
rD <- rsDriver(browser = "firefox", chromever = NULL, verbose = FALSE)

#ethans
#rD <- rsDriver( port = 4445L,browser = "firefox",version = "latest",chromever = NULL, phantomver = NULL)

remDr <- rD[["client"]]

url <- "https://hashtagbasketball.com/fantasy-basketball-rankings"
remDr$navigate(url)
scrape.sleep(c(2, 3))

page <- remDr$getPageSource()[[1]]

tables <- read_html(page) %>%
  html_elements("table") 

#before closing R always run these 
remDr$close()
rD[["server"]]$stop()
```

The table below shows our 2025 point and rank predictions, compared to what has transpired so far this season (except if used during off-season). While, as displayed by the model's tendency to under-predict star players, precisely predicting point production is quite challenging. Rank therefor may be a useful supplementary metric when comparing a players current performance to what the model expected of them. For instance, while Jokic is outperforming his predicted production by about 20%, his predicted rank is only 1 off.

The team changing column on the far right is also something to pay attention to. A new environment and a new roster of teammates can significantly change a players role for better or worse in a way that is very difficult to model.

If a player is performing much better than expected, it could just be a temporary hot streak and likely to fall back down to earth, or it could also be a symptom of a new environment. For instance, Karl-Anthony Towns is performing much better than the model predicted he would. This is likely due to his new role in New York where the team strategy has utilized his skill set very well. Incidentally, RJ Barrett left NYC for Toronto and is also outperforming our model, so obviously its not just the Knicks that make stars better. There are many other such factors that could affect fantasy production that are nigh impossible to model.

These are examples of the qualitative distinctions that need to be considered along with the model. One can combine this qualitative analysis with the statistical power of the model to make decisions on whether a player is worth trading for or trading away if you believe they are under or over valued based on recent events.

While this analysis arrives mid-season, similar predictive modeling before the season could have offered valuable guidance during fantasy drafts, helping managers identify potentially undervalued players and avoid overvalued ones based on historical patterns and projected performance.

```{r}
library(stringi)  #for string normalization

#standardized names
currentrank <- tables[[3]] %>%
  html_table() %>%
  as_tibble() %>%  select(Player = PLAYER, GP = GP,  Points = PTS, Rebounds = TREB, 
         Assists = AST, Steals = STL, Blocks = BLK, Turnovers = TO) %>% 
  filter(Player != "PLAYER") %>% 
  mutate(player_standardized_name = stringi::stri_trans_general(Player, "Latin-ASCII"),
    across(c(GP, Points, Rebounds, Assists, Steals, Blocks, Turnovers), as.numeric),
    FantasyPoints = Points + 1.2 * Rebounds + 1.5 * Assists + 3 * Blocks + 3 * Steals - Turnovers) %>% drop_na()

# Fix names
prediction_data_2025 <- predictions25 %>% 
  mutate(player_standardized_name = stringi::stri_trans_general(player, "Latin-ASCII"),
    PredictedFP = round(.pred, 1)) %>% 
  select(player_standardized_name, PredictedFP)

# Join & fomrat
combinedrank25 <- currentrank %>%
  left_join(prediction_data_2025, by = "player_standardized_name") %>%  
  # Change to 2025 predictions
  select(-player_standardized_name) %>%
  arrange(desc(FantasyPoints)) %>%
  mutate(across(c(FantasyPoints, PredictedFP), ~round(., 1))) %>% 
  distinct(Player, .keep_all = TRUE) 



# First get team info for 2023 and 2024 seasons
team_changes <- historical_stats %>%  
  filter(season %in% c(2024, 2025)) %>%
   mutate(player_standardized_name = stringi::stri_trans_general(player, "Latin-ASCII")) %>%
  select(player_standardized_name, team, season) %>%
  group_by(player_standardized_name) %>%
  summarize(teams = n_distinct(team), new_team = teams > 1) 

# Now join with your ranking data
ranker <- combinedrank25 %>%  
  arrange(desc(FantasyPoints)) %>% 
  mutate(Current_Rank = row_number()) %>% 
  arrange(desc(PredictedFP)) %>% 
  mutate(Predicted_Rank = row_number()) %>%
  mutate(Rank_Change = Predicted_Rank - Current_Rank) %>%
  select(Player, FantasyPoints, PredictedFP, Current_Rank, Predicted_Rank) %>% 
  left_join(team_changes, by = c("Player" = "player_standardized_name")) %>%
  mutate(team_status = case_when(is.na(new_team) ~ "Rookie/Unknown",new_team ~ "New Team",TRUE ~ "Same Team") ) %>%
  arrange(Current_Rank) %>% 
  select(-new_team, -teams) %>% 
  stats::na.omit()

ranker %>%
  kable(col.names = c("Player", "Fantasy Points", "Predicted FP", "Current Rank", "Predicted Rank", "Team Status")
  ) %>%
  kable_styling(full_width = FALSE
  ) %>% 
  scroll_box(height = "400px")
    
```

Final remarks:

In the dynamic world of fantasy basketball, success comes from blending statistical insights with genuine basketball knowledge. This analysis shows that while past performance helps forecast future results, basketball remains very difficult to predict, that's what makes it exciting. The tools and models we've explored here provide a framework for making smarter decisions, especially when it comes to trading players who might be performing above or below their true potential. But remember, these predictions are guides, not guarantees. The best fantasy managers combine data-driven insights with an understanding of the intangibles like team chemistry, player development, and even those impressive hot streaks that can define a season. You're now equipped with both analytical tools, and strategic understanding that put you ahead of the typical fantasy manager. Whether you're joining your first league or aiming to finally claim that championship trophy, you have everything you need to compete at a high level. Good luck.
